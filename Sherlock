#!/usr/bin/python3
# coding: utf-8

# Imports Python
from sys import argv
from os import system


def bringTrue(choice):
    choice = choice.upper().strip()
    if choice == "Y" or choice == "S":
        del choice
        return True
    return False


try:
    import json
    import time
    import requests
    from time import sleep
    from bs4 import BeautifulSoup
    from urllib.error import HTTPError
except:
    print("Deseja instalar os requisitos faltantes? [Y/n]")
    vaiInstalar = input('-> ')
    if bringTrue(vaiInstalar):
        del vaiInstalar
        system("sudo pip3 install -r requirements.txt")
    else:
        print('Sem os requisitos fica díficil, amigo...')
        exit()

# My imports
from parsers.Mails import Mail
from parsers.Files import Files
from utils.Bouncer import Bouncer
from parsers.Images import Images
from parsers.Lattes import Lattes
from parsers.Server import Server
from utils.Arguments import Getopt
from utils.UrlUtils import UrlUtils
from webpkg.WebRequest import WebRequest
from parsers.PhysicalAddr import Physical
from webpkg.PageCreator import PageCreator
from webpkg.ForgeRequest import ForgeRequest
from utils.ExtensionFiles import ExtensionsFile


def printResult(arrayList, message='Elementos: '):
    if arrayList == None or arrayList == []:
        return None
    if type(arrayList) is list and len(arrayList) > 0:
        print(message)
        for element in arrayList:
            if element != None:
                print(element)
    elif type(arrayList) is str:
        print(arrayList)


def foundLinksInPage(codingHTML, bouncer):
    fictUri = []
    linksFound = bouncer.searchAndAddLinksFromMain(codingHTML, bouncer.getDomain())
    for newlink in linksFound:
        if newlink not in fictUri and not ExtensionsFile.hasExtension(newlink):
            if UrlUtils.httpAndWww(newlink) == newlink:
                fictUri.append(newlink)
            else:
                fictUri.append(UrlUtils.httpAndWww(newlink))
                fictUri.append(newlink)
    list(filter('...'.__ne__, fictUri))
    return fictUri


def foundExternalLinks(codingHTML, bouncer):
    fictExt = []
    externalFind = bouncer.searchAndAddExternalPages(codingHTML, bouncer.getDomain())
    for extLink in externalFind:
        if extLink not in fictExt and not ExtensionsFile.hasExtension(extLink):
            if UrlUtils.containsHTTP(extLink) and bouncer.getDomain() not in extLink:
                fictExt.append(extLink)
    list(filter('...'.__ne__, fictExt))
    return fictExt


def msgScreenAtt():
    print('Links externos até o momento: ' + str(len(externalLinks)))
    print('Lattes encontrados até o momento: ' + str(len(curriculo.getLattes())))
    print('Emails encontrados até o momento: ' + str(len(emails.getEmails())))
    print('Imagens encontradas até o momento: ' + str(len(image.getImages())))
    print('Arquivos encontrados até o momento: ' + str(len(files.getFiles())))


def banner():
    print('''\033[1;37m
███████╗██╗  ██╗███████╗██████╗ ██╗      ██████╗  ██████╗██╗  ██╗
██╔════╝██║  ██║██╔════╝██╔══██╗██║     ██╔═══██╗██╔════╝██║ ██╔╝
███████╗███████║█████╗  ██████╔╝██║     ██║   ██║██║     █████╔╝
╚════██║██╔══██║██╔══╝  ██╔══██╗██║     ██║   ██║██║     ██╔═██╗
███████║██║  ██║███████╗██║  ██║███████╗╚██████╔╝╚██████╗██║  ██╗
╚══════╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝ ╚═════╝  ╚═════╝╚═╝  ╚═╝\033[00m
\033[1;37mPython WebSpider for Forensics and Stalkers\033[00m
Developer: \033[1;37mAllan Garcez - vandalvnl\033[00m
GitHub: \033[1;37mhttps://github.com/vandalvnl\033[00m
Sugestões ou Bugs? Por favor reporte: \033[1;37mhttps://github.com/vandalvnl/sherlock/issues\033[00m''')


def ajudaCara():
    print('''\033[1;37mParâmetros de uso: python Sherlock -t site [argumentos opcionais]\033[00m\n
\r\t-b, --both\t\tAtribui uma wordlist e os links buscados nas páginas
\r\t-e, --extensions\tExtensões de arquivos a serem indexados* (em desenvolvimento)
\r\t-i, --invertal\tEspera um intervalo de tempo para a próxima requisição
\r\t-j, --json\tGerar arquivo Json das informações capturadas (deve informar o nome do arquivo)
\r\t-h, --help\t\tMenu de ajuda ao usuário (Mostrado caso 3 ou menos argumentos sejam passados)
\r\t-l, --limit\t\tLimite de requisições feitas até uma pausa.
\r\t-r, --referer\t\tDefine um referer para o cabeçalho HTTP
\r\t-s, --subdomain\t\tRealiza tentativas de subdominio(parâmetro -w obrigatório)
\r\t-t, --target\t\tPágina raiz do site a ser analisado.
\r\t-w, --wordlist\t\tWordlist para tentativa de bruteforce
\n\r*Extensões reconhecidas pelo programa:
\tjpg, jpeg, pdf, png, mp3, mp4, avi, docx, doc, odt, txt,gif, js,
\tcss, csv, exe, mov, psd, tar, zip, wav, xml, xsl, ppt, pptx, m4a,
\togg, m4v, ogv, 3gp, mpg, 3gp, xls\n''')


banner()
if len(argv) < 3:
    ajudaCara()
elif ('-h', '--help') in argv:
    ajudaCara()

uri = ''
html = ''
limit = ''
wordlist = ''
jsonFile = ''
validLinks = []
externalLinks = []
url = Getopt.requiredArgs(argv, ['-t', '--target']).strip()
if url[-1] != '/' and not ExtensionsFile.extensionForWebPage(url):
    url += '/'
if Getopt.getOpt(argv, ['-l', '--limit']):
    limit = int(Getopt.getOptAndValue(argv, ['-l', '--limit']).strip())
else:
    limit = False
if Getopt.getOpt(argv, ['-L', '--language']):
    language = Getopt.getOptAndValue(argv, ['-L', '--language']).strip().split(',')
else:
    language = 'pt-br'
if Getopt.getOpt(argv, ['-j', '--json']):
    jsonFile = Getopt.getOptAndValue(argv, ['-j', '--json']).strip()
else:
    jsonFile = False
if Getopt.getOpt(argv, ['-i', '--interval']):
    interval = int(Getopt.getOptAndValue(argv, ['-i', '--invertal']).strip())
else:
    interval = 0
if Getopt.getOpt(argv, ['-r', '--referer']):
    referer = Getopt.getOptAndValue(argv, ['-r', '--referer']).strip().split(',')
else:
    referer = None
wordlistOpt = Getopt.getOpt(argv, ['-w', '--wordlist'])
bothThem = Getopt.getOpt(argv, ['-b', '--both'])
saltos = limit
bouncer = Bouncer(url)
zipcode = Physical()
image = Images()
curriculo = Lattes()
emails = Mail()
files = Files()
uri = [url]
if wordlistOpt:
    wordlist = Getopt.getOptAndValue(argv, ['-w', '--wordlist']).strip()
    bouncer.setWordlist(wordlist)
    while (uri[-1] != None):
        try:
            if Getopt.getOpt(argv, ['-s', '--subdomain']):
                subdomain = 'http://' + bouncer.getCurrentWordFromWordlist() + '.' + bouncer.getDomain()
                uri.append(subdomain.strip())
            add = url + bouncer.getCurrentWordFromWordlist()
            uri.append(add.strip())
        except TypeError:
            continue

list(filter(None.__ne__, uri))

initialTime = time.time()

for link in uri:
    try:
        try:
            request = ForgeRequest.makeFakeRequest(link, referer, language)
        except requests.exceptions.InvalidSchema:
            continue
        if WebRequest.isActiveLink(link):
            html = request.text
            beautifulHTML = BeautifulSoup(html, 'html.parser').prettify()
            codingHTML = BeautifulSoup(html, 'html.parser')
            if bothThem:
                tmpUri = foundLinksInPage(codingHTML, bouncer)
            else:
                tmpUri = foundLinksInPage(codingHTML, bouncer)
            tmpExternal = foundExternalLinks(codingHTML, bouncer)
            zipcode.setCep(beautifulHTML)
            emails.setEmails(beautifulHTML)
            curriculo.setLattes(beautifulHTML)
            image.setImages(codingHTML, link)
            files.setFiles(codingHTML, link)
            validLinks.append(link)
            print('--------------------------------------------------------')
            print('Informação sobre: ' + link)
            msgScreenAtt()
            for validLink in tmpUri:
                if type(validLink) is str and validLink not in uri:
                    uri.append(validLink)
            for validExternalLink in tmpExternal:
                if type(validExternalLink) is str and validExternalLink not in externalLinks:
                    externalLinks.append(validExternalLink)
            del tmpExternal, tmpUri
            saltos -= 1
            if saltos == 0:
                saltos = limit
                print('--------------------------------------------------------')
                print('Deseja continuar com as buscas?[Y/n] ')
                doContinue = input('-> ')
                if not bringTrue(doContinue):
                    break
            sleep(interval)
    except (requests.ConnectionError, requests.exceptions.MissingSchema):
        continue
    except KeyboardInterrupt:
        break
endTime = time.time()

print()
print('Fim da análise formatando resultados...Tempo gasto: ' + str(round(endTime - initialTime, 3)) + '  segundos')
try:
    position = Server('www.' + bouncer.getDomain())
except:
    pass

if bringTrue(input('Deseja imprimir o resultado da analise? [S/n] ')):
    for cep in zipcode.getAllCeps():
        print(zipcode.getAddress(cep))
    print()
    print(position.getGeoLocation())
    print()
    emailsSort = emails.getEmails()
    emailsSort.sort()
    printResult(emailsSort, 'Lista de Emails:\n')
    print()
    curriculoSort = curriculo.getLattes()
    curriculoSort.sort()
    printResult(curriculoSort, 'Lista de Currículos Lattes:\n')
    print()
    imageSort = image.getImages()
    imageSort.sort()
    printResult(image.getImages(), 'Lista de Imagens:\n')
    print()
    sortFiles = files.getFiles()
    sortFiles.sort()
    printResult(sortFiles, 'Lista de Arquivos:\n ')
    print()
    externalLinks.sort()
    printResult(externalLinks, 'Lista de links externos:\n')

if jsonFile is not False:
    jsonFile = jsonFile.replace('.json', '')
    jsonFile += '.json'
    try:
        archive = open(jsonFile, 'w')
    except:
        print('Erro ao criar arquivo' + jsonFile)
        exit()
    archive.write(json.dumps({
        'emails': emailsSort,
        'lattes': curriculoSort,
        'domain': bouncer.getDomain(),
        'server-location': position.getGeoLocation(),
        'address': zipcode.getAllCeps(),
        'image': imageSort,
        'files': sortFiles,
        'external': externalLinks
    }, sort_keys=True, indent=3))
    archive.close()
    print("Arquivo " + jsonFile + " gerado com sucesso")
    del jsonFile, archive

if Getopt.getOpt(argv, ['-H', '--html']):
    directory = Getopt.getOptAndValue(argv, ['-H', '--html']).strip()
    index = PageCreator(directory, bouncer.getDomain(), position.getIp(url), zipcode)
    index.createIndex(
        emailsSort, curriculoSort, sortFiles, imageSort, validLinks, externalLinks
    )
    index.createEmailsPage(emailsSort)
    index.createImgPage(imageSort)
    index.createFilesPage(sortFiles)
    index.createLattesPage(curriculoSort)
    index.createUrlsPage(validLinks)
    index.createExternalPage(externalLinks)
    print("Todos os HTMLS foram gerados com sucesso")
    del directory, index, emails, curriculo, files, image, validLinks
    del sortFiles, curriculoSort, curriculoSort, imageSort, emailsSort
del uri, url, externalLinks, wordlist, json, limit, html, language, interval, wordlistOpt
del bothThem, saltos, bouncer, zipcode, image, curriculo, emails, files