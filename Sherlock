#!/usr/bin/python3.6
# coding: utf-8


# Imports Python
from sys import argv
from os import system


def bringTrue(choice):
    choice = choice.upper().strip()
    if choice == "Y" or choice == "S":
        return True
    return False


try:
    import json
    import time
    import requests
    from bs4 import BeautifulSoup
    from urllib.error import HTTPError
    from urllib.request import urlopen
except:
    print("Deseja instalar os requisitos faltantes? [Y/n]")
    vaiInstalar = input('-> ')
    if bringTrue(vaiInstalar):
        system("sudo pip3 install -r requirements.txt")
    else:
        print('Sem os requisitos fica díficil, amigo...')
        exit()

# My imports
from parsers.Mails import Mail
from parsers.Files import Files
from utils.Bouncer import Bouncer
from parsers.Images import Images
from parsers.Lattes import Lattes
from parsers.Server import Server
from utils.Arguments import Getopt
from webpkg.WebRequest import WebRequest
from parsers.PhysicalAddr import Physical
from webpkg.PageCreator import PageCreator
from utils.ExtensionFiles import ExtensionsFile


def printResult(arrayList):
    if arrayList == None or arrayList == []:
        return None
    for element in arrayList:
        if element != None:
            print(element)



def foundLinksInPage(uri, codingHTML, bouncer):
    linksFound = bouncer.searchAndAddLinksFromMain(codingHTML, bouncer.getDomain())
    for newlink in linksFound:
        if newlink not in uri and not ExtensionsFile.hasExtension(newlink):
            uri.append(newlink)
    return uri


def banner():
    print('''\033[1;37m
███████╗██╗  ██╗███████╗██████╗ ██╗      ██████╗  ██████╗██╗  ██╗
██╔════╝██║  ██║██╔════╝██╔══██╗██║     ██╔═══██╗██╔════╝██║ ██╔╝
███████╗███████║█████╗  ██████╔╝██║     ██║   ██║██║     █████╔╝
╚════██║██╔══██║██╔══╝  ██╔══██╗██║     ██║   ██║██║     ██╔═██╗
███████║██║  ██║███████╗██║  ██║███████╗╚██████╔╝╚██████╗██║  ██╗
╚══════╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝ ╚═════╝  ╚═════╝╚═╝  ╚═╝\033[00m
\033[1;37mPython WebSpider for Forensics and Stalkers\033[00m
Developer: \033[1;37mAllan Garcez - vandalvnl\033[00m
GitHub: \033[1;37mhttps://github.com/vandalvnl\033[00m
Sugestões ou Bugs? Por favor reporte: \033[1;37mhttps://github.com/vandalvnl/sherlock/issues\033[00m''')


def ajudaCara():
    print('''\033[1;37mParâmetros de uso: python Sherlock -t site [argumentos opcionais]\033[00m\n
\r\t-b, --both\t\tAtribui uma wordlist e os links buscados nas páginas
\r\t-e, --extensions\tExtensões de arquivos a serem indexados* (em desenvolvimento)
\r\t-j, --json\tGerar arquivo Json das informações capturadas (deve informar o nome do arquivo)
\r\t-h, --help\t\tMenu de ajuda ao usuário (Mostrado caso 3 ou menos argumentos sejam passados)
\r\t-l, --limit\t\tLimite de requisições feitas até uma pausa.
\r\t-s, --subdomain\t\tRealiza tentativas de subdominio(parâmetro -w obrigatório)
\r\t-t, --target\t\tPágina raiz do site a ser analisado.
\r\t-w, --wordlist\t\tWordlist para tentativa de bruteforce
\n\r*Extensões reconhecidas pelo programa:
\tjpg, jpeg, pdf, png, mp3, mp4, avi, docx, doc, odt, txt,gif, js,
\tcss, csv, exe, mov, psd, tar, zip, wav, xml, xsl, ppt, pptx, m4a,
\togg, m4v, ogv, 3gp, mpg, 3gp, xls\n''')

banner()
if len(argv) < 3:
    ajudaCara()
elif ('-h','--help') in argv:
    ajudaCara()

uri = ''
html = ''
limit = ''
wordlist=''
jsonFile = ''
validLinks = []
url = Getopt.requiredArgs(argv, ['-t', '--target']).strip()
if url[-1] != '/' and not ExtensionsFile.extensionForWebPage(url):
    url += '/'
if Getopt.getOpt(argv, ['-l', '--limit']):
    limit = int(Getopt.getOptAndValue(argv, ['-l', '--limit']).strip())
else:
    limit = False
if Getopt.getOpt(argv, ['-j', '--json']):
    jsonFile = Getopt.getOptAndValue(argv, ['-j', '--json']).strip()
else:
    jsonFile = False
wordlistOpt = Getopt.getOpt(argv, ['-w','--wordlist'])
bothThem = Getopt.getOpt(argv, ['-b','--both'])
saltos = limit
bouncer = Bouncer(url)
zipcode = Physical()
image = Images()
curriculo = Lattes()
emails = Mail()
files = Files()

uri = [url]
if wordlistOpt:
    wordlist = Getopt.getOptAndValue(argv, ['-w','--wordlist']).strip()
    bouncer.setWordlist(wordlist)
    while(uri[-1] != None):
        try:
            if Getopt.getOpt(argv, ['-s','--subdomain']):
                subdomain = 'http://' + bouncer.getCurrentWordFromWordlist() + '.' + bouncer.getDomain()
                uri.append(subdomain.strip())
            add = url + bouncer.getCurrentWordFromWordlist()
            uri.append(add.strip())
        except TypeError:
            break

list(filter(None.__ne__, uri))

initialTime = time.time()
for link in uri:
    try:
        try:
            request = requests.get(link, headers = WebRequest.makeHeaderHTTP(link))
        except requests.exceptions.InvalidSchema:
            continue
        if WebRequest.isActiveLink(link):
            html = request.text
            beautifulHTML = BeautifulSoup(html, 'html.parser').prettify()
            codingHTML = BeautifulSoup(html, 'html.parser')

            if bothThem:
                uri.append(foundLinksInPage(uri, codingHTML, bouncer))
            elif wordlistOpt:
                pass
            else:
                uri.append(foundLinksInPage(uri, codingHTML, bouncer))

            zipcode.setCep(beautifulHTML)
            emails.setEmails(beautifulHTML)
            curriculo.setLattes(beautifulHTML)
            image.setImages(codingHTML, link)
            files.setFiles(codingHTML, link)
            validLinks.append(link)
            print('--------------------------------------------------------')
            print(f'Informação sobre: {link}')
            print(f'Emails encontrados até o momento: {str(len(emails.getEmails()))}')
            print(f'Lattes encontrados até o momento: {str(len(curriculo.getLattes()))}')
            print(f'Imagens encontradas até o momento: {str(len(image.getImages()))}')
            print(f'Arquivos encontrados até o momento: {str(len(files.getFiles()))}')
            saltos -= 1
            if saltos == 0:
                saltos = limit
                print('Deseja continuar com as buscas?[Y/n]')
                doContinue = input('-> ')
                if not bringTrue(doContinue):
                    break
    except (requests.ConnectionError, requests.exceptions.MissingSchema):
        continue
    except KeyboardInterrupt:
        break
endTime = time.time()

print()
print(f'Fim da análise formatando resultados...Tempo gasto: {round(endTime-initialTime, 3)} segundos')
try:
    position = Server('www.' + bouncer.getDomain())
except:
    pass

if bringTrue(input('Deseja imprimir o resultado da analise? [S/n]')):
    print(bouncer.getDomain())
    print()
    for cep in zipcode.getAllCeps():
        print(zipcode.getAddress(cep))
    print()
    print(position.getGeoLocation())
    print()
    printResult(emails.getEmails())
    print()
    if len(curriculo.getLattes()) > 0:
        printResult(curriculo.getLattes())
        print()
    printResult(image.getImages())
    printResult(files.getFiles())

if jsonFile is not False:
    jsonFile = jsonFile.replace('.json','')
    jsonFile += '.json'
    try:
        archive = open(jsonFile, 'w')
    except:
        print(f'Erro ao criar arquivo {jsonFile}')
        exit()
    archive.write(json.dumps({
        'emails': emails.getEmails(),
        'lattes': curriculo.getLattes(),
        'domain': bouncer.getDomain(),
        'server-location': position.getGeoLocation(),
        'address': zipcode.getAllCeps(),
        'image': image.getImages(),
        'files': files.getFiles()
    }, sort_keys = True, indent = 2))
    archive.close()
    print(f"Arquivo {jsonFile} gerado com sucesso")

if Getopt.getOpt(argv, ['-H', '--html']):
    directory = Getopt.getOptAndValue(argv, ['-H', '--html']).strip()
    index = PageCreator(directory, bouncer.getDomain(), position.getIp(url), zipcode)
    index.createIndex(
        emails.getEmails(), curriculo.getLattes(), files.getFiles(), image.getImages(), validLinks
    )
    index.createEmailsPage(emails.getEmails())
    index.createImgPage(image.getImages())
    index.createFilesPage(files.getFiles())
    index.createLattesPage(curriculo.getLattes())
    index.createUrlsPage(validLinks)
    print("Todos os HTMLS foram gerados com sucesso")
